{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95952d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: gym in /usr/local/lib/python3.10/site-packages (0.26.2)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/site-packages (from gym) (1.24.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.10/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/site-packages (from gym[classic_control]) (1.24.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/site-packages (from gym[classic_control]) (2.2.1)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/site-packages (from gym[classic_control]) (0.0.8)\n",
      "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.10/site-packages (from gym[classic_control]) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "#Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install gym\n",
    "!{sys.executable} -m pip install gym[classic_control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e1d4a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=16):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        action_probs = self.actor(state)\n",
    "        state_value = self.critic(state)\n",
    "        return action_probs, state_value\n",
    "\n",
    "def compute_returns(rewards, gamma):\n",
    "    R = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f40fe934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nenv = gym.make(\"CartPole-v1\")\\ninput_dim = env.observation_space.shape[0]\\noutput_dim = env.action_space.n\\nmodel = ActorCritic(input_dim, output_dim)\\nsteps_per_batch = 50\\n\\nstates, actions, log_probs, returns, rewards = rollout(steps_per_batch, model, env, gamma=0.99)\\n'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def rollout(steps_per_batch, model, env, gamma):\n",
    "    states, actions, log_probs, returns, rewards = [], [], [], [], []\n",
    "    \n",
    "    t = 0\n",
    "    n_eps = 0\n",
    "    while t < steps_per_batch:\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        eps_rewards = []\n",
    "\n",
    "        while not done:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action_probs, _ = model(state)\n",
    "            distribution = Categorical(action_probs)\n",
    "\n",
    "            action = distribution.sample()\n",
    "            log_prob = distribution.log_prob(action)\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action.item())\n",
    "\n",
    "            eps_rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            \n",
    "            state = next_state\n",
    "\n",
    "        returns.extend(compute_returns(eps_rewards, gamma))\n",
    "        rewards.extend(eps_rewards)\n",
    "        t += len(returns)\n",
    "        n_eps += 1\n",
    "\n",
    "    return states, actions, log_probs, returns, rewards, n_eps\n",
    "\n",
    "\"\"\"\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "input_dim = env.observation_space.shape[0]\n",
    "output_dim = env.action_space.n\n",
    "model = ActorCritic(input_dim, output_dim)\n",
    "steps_per_batch = 50\n",
    "\n",
    "states, actions, log_probs, returns, rewards = rollout(steps_per_batch, model, env, gamma=0.99)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "10a08bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Eps count: 13 Reawrds: 297.0000 Policy Loss: -0.0032, Value Loss: 10.5298\n",
      "Epoch: 10, Eps count: 13 Reawrds: 366.0000 Policy Loss: -0.0034, Value Loss: 10.8348\n",
      "Epoch: 20, Eps count: 11 Reawrds: 343.0000 Policy Loss: 0.0002, Value Loss: 10.9143\n",
      "Epoch: 30, Eps count: 15 Reawrds: 318.0000 Policy Loss: -0.0025, Value Loss: 10.1740\n",
      "Epoch: 40, Eps count: 14 Reawrds: 332.0000 Policy Loss: -0.0049, Value Loss: 10.3904\n",
      "Epoch: 50, Eps count: 14 Reawrds: 277.0000 Policy Loss: -0.0011, Value Loss: 10.0351\n",
      "Epoch: 60, Eps count: 14 Reawrds: 294.0000 Policy Loss: -0.0024, Value Loss: 10.0615\n",
      "Epoch: 70, Eps count: 13 Reawrds: 332.0000 Policy Loss: -0.0013, Value Loss: 10.6247\n",
      "Epoch: 80, Eps count: 14 Reawrds: 282.0000 Policy Loss: -0.0070, Value Loss: 10.1615\n",
      "Epoch: 90, Eps count: 14 Reawrds: 302.0000 Policy Loss: 0.0026, Value Loss: 10.2234\n",
      "Epoch: 100, Eps count: 14 Reawrds: 394.0000 Policy Loss: -0.0057, Value Loss: 10.8770\n",
      "Epoch: 110, Eps count: 15 Reawrds: 251.0000 Policy Loss: -0.0036, Value Loss: 9.7020\n",
      "Epoch: 120, Eps count: 14 Reawrds: 261.0000 Policy Loss: -0.0037, Value Loss: 9.7576\n",
      "Epoch: 130, Eps count: 16 Reawrds: 320.0000 Policy Loss: -0.0009, Value Loss: 10.0388\n",
      "Epoch: 140, Eps count: 13 Reawrds: 305.0000 Policy Loss: -0.0042, Value Loss: 10.4507\n",
      "Epoch: 150, Eps count: 12 Reawrds: 356.0000 Policy Loss: -0.0007, Value Loss: 10.7556\n",
      "Epoch: 160, Eps count: 14 Reawrds: 282.0000 Policy Loss: -0.0007, Value Loss: 9.9313\n",
      "Epoch: 170, Eps count: 13 Reawrds: 290.0000 Policy Loss: -0.0001, Value Loss: 10.2457\n",
      "Epoch: 180, Eps count: 12 Reawrds: 303.0000 Policy Loss: -0.0030, Value Loss: 10.5312\n",
      "Epoch: 190, Eps count: 12 Reawrds: 349.0000 Policy Loss: -0.0015, Value Loss: 10.5805\n",
      "Epoch: 200, Eps count: 13 Reawrds: 344.0000 Policy Loss: -0.0026, Value Loss: 10.7161\n",
      "Epoch: 210, Eps count: 13 Reawrds: 288.0000 Policy Loss: -0.0026, Value Loss: 10.2826\n",
      "Epoch: 220, Eps count: 13 Reawrds: 285.0000 Policy Loss: -0.0004, Value Loss: 10.4019\n",
      "Epoch: 230, Eps count: 15 Reawrds: 294.0000 Policy Loss: -0.0023, Value Loss: 10.0320\n",
      "Epoch: 240, Eps count: 15 Reawrds: 294.0000 Policy Loss: -0.0036, Value Loss: 9.9881\n",
      "Epoch: 250, Eps count: 12 Reawrds: 308.0000 Policy Loss: 0.0002, Value Loss: 10.4771\n",
      "Epoch: 260, Eps count: 13 Reawrds: 281.0000 Policy Loss: -0.0016, Value Loss: 10.3119\n",
      "Epoch: 270, Eps count: 14 Reawrds: 366.0000 Policy Loss: -0.0074, Value Loss: 10.5956\n",
      "Epoch: 280, Eps count: 13 Reawrds: 337.0000 Policy Loss: -0.0044, Value Loss: 10.5271\n",
      "Epoch: 290, Eps count: 13 Reawrds: 316.0000 Policy Loss: 0.0022, Value Loss: 10.2033\n",
      "Epoch: 300, Eps count: 15 Reawrds: 309.0000 Policy Loss: -0.0027, Value Loss: 10.1357\n",
      "Epoch: 310, Eps count: 15 Reawrds: 266.0000 Policy Loss: -0.0004, Value Loss: 9.7203\n",
      "Epoch: 320, Eps count: 13 Reawrds: 294.0000 Policy Loss: -0.0008, Value Loss: 10.3667\n",
      "Epoch: 330, Eps count: 14 Reawrds: 285.0000 Policy Loss: -0.0050, Value Loss: 10.1714\n",
      "Epoch: 340, Eps count: 13 Reawrds: 292.0000 Policy Loss: -0.0020, Value Loss: 10.1294\n",
      "Epoch: 350, Eps count: 14 Reawrds: 284.0000 Policy Loss: -0.0041, Value Loss: 10.1129\n",
      "Epoch: 360, Eps count: 13 Reawrds: 280.0000 Policy Loss: 0.0014, Value Loss: 10.1674\n",
      "Epoch: 370, Eps count: 14 Reawrds: 293.0000 Policy Loss: -0.0013, Value Loss: 10.1387\n",
      "Epoch: 380, Eps count: 14 Reawrds: 293.0000 Policy Loss: -0.0025, Value Loss: 10.1433\n",
      "Epoch: 390, Eps count: 14 Reawrds: 281.0000 Policy Loss: 0.0009, Value Loss: 9.8667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def train_ppo(\n",
    "        env_name=\"CartPole-v1\", gamma=0.7, lr=1e-6,\n",
    "        clip_epsilon=0.2,epochs=400, num_updates=10,\n",
    "        steps_per_batch=2048):\n",
    "    env = gym.make(env_name)\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    model = ActorCritic(input_dim, output_dim)\n",
    "    policy_optimizer = optim.Adam(model.actor.parameters(), lr=lr)\n",
    "    value_optimizer = optim.Adam(model.critic.parameters(), lr=lr)\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        states, actions, log_probs, returns, rewards, n_eps = rollout(steps_per_batch, model, env, gamma)\n",
    "    \n",
    "        states = torch.stack(states) \n",
    "        actions = torch.stack(actions)\n",
    "        returns = torch.FloatTensor(returns).unsqueeze(0)\n",
    "        log_probs = torch.FloatTensor(log_probs).unsqueeze(0)\n",
    "        for _ in range(5):\n",
    "            values = model.critic(states).squeeze().unsqueeze(0)\n",
    "\n",
    "            cur_action_probs = model.actor(states)\n",
    "            dist = Categorical(cur_action_probs)\n",
    "            cur_log_probs = dist.log_prob(actions)\n",
    "\n",
    "            advantage = returns - values.detach()\n",
    "            advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-05)\n",
    "\n",
    "            ratio = (cur_log_probs - log_probs.detach()).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantage\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            policy_optimizer.zero_grad()\n",
    "            policy_loss.backward(retain_graph=True)\n",
    "            policy_optimizer.step()\n",
    "\n",
    "            value_loss = nn.MSELoss(reduction=\"mean\")(returns, values)\n",
    "\n",
    "            value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            value_optimizer.step()\n",
    "\n",
    "        if epoch % num_updates == 0:\n",
    "            print(\"Epoch: {}, Eps count: {} Reawrds: {:.4f} Policy Loss: {:.4f}, Value Loss: {:.4f}\".format(epoch, n_eps, sum(rewards), policy_loss.item(), value_loss.item()))\n",
    "\n",
    "    env.close()\n",
    "\n",
    "train_ppo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afe38565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(env.observation_space.shape)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a53f4405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5742980467215641"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.log(0.5631)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b51dda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
