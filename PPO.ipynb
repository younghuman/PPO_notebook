{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95952d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: gym in /usr/local/lib/python3.10/site-packages (0.26.2)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/site-packages (from gym) (0.0.8)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/site-packages (from gym) (2.2.1)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/site-packages (from gym) (1.24.1)\n",
      "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.10/site-packages (0.26.2)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/site-packages (from gym[classic_control]) (0.0.8)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/site-packages (from gym[classic_control]) (1.24.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/site-packages (from gym[classic_control]) (2.2.1)\n",
      "Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.10/site-packages (from gym[classic_control]) (2.1.0)\n",
      "Requirement already satisfied: moviepy==1.0.3 in /usr/local/lib/python3.10/site-packages (1.0.3)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/site-packages (from moviepy==1.0.3) (2.28.1)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/site-packages (from moviepy==1.0.3) (0.1.10)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/site-packages (from moviepy==1.0.3) (2.26.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/site-packages (from moviepy==1.0.3) (1.24.1)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/site-packages (from moviepy==1.0.3) (4.4.2)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/site-packages (from moviepy==1.0.3) (0.4.8)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/site-packages (from moviepy==1.0.3) (4.64.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/site-packages (from imageio<3.0,>=2.5->moviepy==1.0.3) (9.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (2.1.1)\n",
      "Requirement already satisfied: ffmpeg in /usr/local/lib/python3.10/site-packages (1.4)\n"
     ]
    }
   ],
   "source": [
    "#Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install gym --upgrade\n",
    "!{sys.executable} -m pip install gym[classic_control] --upgrade\n",
    "!{sys.executable} -m pip install moviepy==1.0.3 --upgrade\n",
    "!{sys.executable} -m pip install ffmpeg --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1d4a037",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=16):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        action_probs = self.actor(state)\n",
    "        state_value = self.critic(state)\n",
    "        return action_probs, state_value\n",
    "\n",
    "def compute_returns(rewards, gamma):\n",
    "    R = 0\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2896347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(steps_per_batch, model, env, gamma, max_steps_before_done = 1024):\n",
    "    states, actions, log_probs, returns, rewards = [], [], [], [], []\n",
    "    \n",
    "    t = 0\n",
    "    n_eps = 0\n",
    "    while t < steps_per_batch:\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        eps_rewards = []\n",
    "        \n",
    "        cur_t = 0\n",
    "        while done is False and cur_t < max_steps_before_done:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action_probs, _ = model(state)\n",
    "            distribution = Categorical(action_probs)\n",
    "\n",
    "            action = distribution.sample()\n",
    "            log_prob = distribution.log_prob(action)\n",
    "\n",
    "            next_state, reward, done, _, _ = env.step(action.item())\n",
    "\n",
    "            eps_rewards.append(reward)\n",
    "            log_probs.append(log_prob)\n",
    "            # if done: print(reward)\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            \n",
    "            state = next_state\n",
    "            cur_t += 1\n",
    "\n",
    "        returns.extend(compute_returns(eps_rewards, gamma))\n",
    "        rewards.append(eps_rewards)\n",
    "        t += len(returns)\n",
    "        n_eps += 1\n",
    "\n",
    "    return states, actions, log_probs, returns, rewards, n_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10a08bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Eps count: 7 Reawrds: 18.8571 Policy Loss: -0.0018, Value Loss: 135.9920\n",
      "Epoch: 100, Eps count: 3 Reawrds: 135.0000 Policy Loss: -0.0016, Value Loss: 2086.0027\n",
      "Epoch: 200, Eps count: 2 Reawrds: 238.5000 Policy Loss: -0.0009, Value Loss: 2603.6438\n",
      "Epoch: 300, Eps count: 1 Reawrds: 1002.0000 Policy Loss: -0.0004, Value Loss: 2608.9373\n",
      "Epoch: 400, Eps count: 2 Reawrds: 699.5000 Policy Loss: -0.0012, Value Loss: 983.3181\n",
      "Epoch: 500, Eps count: 1 Reawrds: 1024.0000 Policy Loss: -0.0008, Value Loss: 516.5413\n",
      "Epoch: 600, Eps count: 1 Reawrds: 1024.0000 Policy Loss: -0.0008, Value Loss: 405.1033\n",
      "Epoch: 700, Eps count: 1 Reawrds: 1024.0000 Policy Loss: -0.0002, Value Loss: 387.0038\n",
      "Epoch: 800, Eps count: 1 Reawrds: 761.0000 Policy Loss: -0.0005, Value Loss: 509.6077\n",
      "Epoch: 900, Eps count: 1 Reawrds: 1024.0000 Policy Loss: -0.0000, Value Loss: 392.6066\n",
      "Epoch: 1000, Eps count: 1 Reawrds: 1024.0000 Policy Loss: -0.0001, Value Loss: 384.0493\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean \n",
    "import copy\n",
    "\n",
    "def train_ppo(\n",
    "        env_name=\"CartPole-v1\", gamma=0.99, lr=5e-4,\n",
    "        clip_epsilon=0.2, epochs=1001, num_updates=100,\n",
    "        steps_per_batch=512, updates_per_epoch=5):\n",
    "    env = gym.make(env_name)\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    old_models = []\n",
    "    model = ActorCritic(input_dim, output_dim, hidden_dim = 32)\n",
    "    policy_optimizer = optim.Adam(model.actor.parameters(), lr=lr)\n",
    "    value_optimizer = optim.Adam(model.critic.parameters(), lr=lr)\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        states, actions, log_probs, returns, rewards, n_eps = play(steps_per_batch, model, env, gamma)\n",
    "    \n",
    "        states = torch.stack(states) \n",
    "        actions = torch.stack(actions)\n",
    "        returns = torch.FloatTensor(returns).unsqueeze(1)\n",
    "        log_probs = torch.FloatTensor(log_probs).unsqueeze(1)\n",
    "\n",
    "        for _ in range(updates_per_epoch):\n",
    "            values = model.critic(states).squeeze().unsqueeze(1)\n",
    "            \n",
    "            cur_action_probs = model.actor(states)\n",
    "            dist = Categorical(cur_action_probs)\n",
    "            cur_log_probs = dist.log_prob(actions).squeeze().unsqueeze(1)\n",
    "\n",
    "            advantage = (returns - values.detach()).squeeze().unsqueeze(1)\n",
    "            advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-5)\n",
    "            \n",
    "\n",
    "            ratio = (cur_log_probs - log_probs.detach()).exp()\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantage\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            policy_optimizer.zero_grad()\n",
    "            policy_loss.backward(retain_graph=True)\n",
    "            policy_optimizer.step()\n",
    "  \n",
    "            value_loss = nn.MSELoss(reduction=\"mean\")(returns, values)\n",
    "\n",
    "            value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            value_optimizer.step()\n",
    "\n",
    "        if epoch % num_updates == 0:\n",
    "            print(\"Epoch: {}, Eps count: {} Reawrds: {:.4f} Policy Loss: {:.4f}, Value Loss: {:.4f}\".format(epoch, n_eps, mean(sum(r) for r in rewards), policy_loss.item(), value_loss.item()))\n",
    "            old_model = copy.deepcopy(model)\n",
    "            old_models.append(old_model)\n",
    "    env.close()\n",
    "    return old_models\n",
    "\n",
    "models = train_ppo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afe38565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/hiya/Code/GPT_Learning/video/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/hiya/Code/GPT_Learning/video/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/hiya/Code/GPT_Learning/video/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/hiya/Code/GPT_Learning/video/rl-video-episode-1.mp4.\n",
      "Moviepy - Writing video /Users/hiya/Code/GPT_Learning/video/rl-video-episode-1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/hiya/Code/GPT_Learning/video/rl-video-episode-1.mp4\n",
      "Moviepy - Building video /Users/hiya/Code/GPT_Learning/video/rl-video-episode-2.mp4.\n",
      "Moviepy - Writing video /Users/hiya/Code/GPT_Learning/video/rl-video-episode-2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/hiya/Code/GPT_Learning/video/rl-video-episode-2.mp4\n",
      "Moviepy - Building video /Users/hiya/Code/GPT_Learning/video/rl-video-episode-3.mp4.\n",
      "Moviepy - Writing video /Users/hiya/Code/GPT_Learning/video/rl-video-episode-3.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/hiya/Code/GPT_Learning/video/rl-video-episode-3.mp4\n",
      "Moviepy - Building video /Users/hiya/Code/GPT_Learning/video/rl-video-episode-4.mp4.\n",
      "Moviepy - Writing video /Users/hiya/Code/GPT_Learning/video/rl-video-episode-4.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/hiya/Code/GPT_Learning/video/rl-video-episode-4.mp4\n",
      "Moviepy - Building video /Users/hiya/Code/GPT_Learning/video/rl-video-episode-5.mp4.\n",
      "Moviepy - Writing video /Users/hiya/Code/GPT_Learning/video/rl-video-episode-5.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/hiya/Code/GPT_Learning/video/rl-video-episode-5.mp4\n",
      "Moviepy - Building video /Users/hiya/Code/GPT_Learning/video/rl-video-episode-6.mp4.\n",
      "Moviepy - Writing video /Users/hiya/Code/GPT_Learning/video/rl-video-episode-6.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/hiya/Code/GPT_Learning/video/rl-video-episode-6.mp4\n",
      "Moviepy - Building video /Users/hiya/Code/GPT_Learning/video/rl-video-episode-7.mp4.\n",
      "Moviepy - Writing video /Users/hiya/Code/GPT_Learning/video/rl-video-episode-7.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/hiya/Code/GPT_Learning/video/rl-video-episode-7.mp4\n",
      "Moviepy - Building video /Users/hiya/Code/GPT_Learning/video/rl-video-episode-8.mp4.\n",
      "Moviepy - Writing video /Users/hiya/Code/GPT_Learning/video/rl-video-episode-8.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/hiya/Code/GPT_Learning/video/rl-video-episode-8.mp4\n",
      "Moviepy - Building video /Users/hiya/Code/GPT_Learning/video/rl-video-episode-9.mp4.\n",
      "Moviepy - Writing video /Users/hiya/Code/GPT_Learning/video/rl-video-episode-9.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/hiya/Code/GPT_Learning/video/rl-video-episode-9.mp4\n",
      "Moviepy - Building video /Users/hiya/Code/GPT_Learning/video/rl-video-episode-10.mp4.\n",
      "Moviepy - Writing video /Users/hiya/Code/GPT_Learning/video/rl-video-episode-10.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/hiya/Code/GPT_Learning/video/rl-video-episode-10.mp4\n"
     ]
    }
   ],
   "source": [
    "from gym.wrappers import RecordVideo\n",
    "env = RecordVideo(gym.make('CartPole-v1', render_mode=\"rgb_array\"), \"./video\", episode_trigger = lambda x: True)\n",
    "\n",
    "for model in models:\n",
    "    state, _ = env.reset()\n",
    "    state = torch.FloatTensor(state)\n",
    "    total_reward = 0\n",
    "    cur_t = 0\n",
    "    done = False\n",
    "    while done is False:\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)        \n",
    "        action_probs, _ = model(state)\n",
    "        # Deterministic action selection use the line below\n",
    "        #action = torch.argmax(action_probs.squeeze())\n",
    "        \n",
    "        distribution = Categorical(action_probs)\n",
    "        action = distribution.sample()\n",
    "        \n",
    "        next_state, reward, done, _, _ = env.step(action.item())\n",
    "\n",
    "        cur_t += 1\n",
    "        state = next_state\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15168deb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
